{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AI_Lab4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/markonium/AI_Pattern_Recognition/blob/master/Movies_Reviews_Classification_Using_BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 929
        },
        "id": "ON86YEEmui1Q",
        "outputId": "3868dd26-f700-4849-edb8-718a74beb49f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.16.2-py3-none-any.whl (3.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 34.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 4.3 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 31.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting tokenizers!=0.11.3,>=0.10.1\n",
            "  Downloading tokenizers-0.11.4-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.8 MB 30.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.10.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-7dee2e6d7053>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBertModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mUSE_GLOBAL_DEPS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0m_load_global_deps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;31m# Appease the type checker; ordinarily this binding is inserted by the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: KeyboardInterrupt: "
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, confusion_matrix, precision_recall_fscore_support\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nltk.tokenize import RegexpTokenizer, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "!pip install transformers\n",
        "import nltk\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from torch import nn, utils\n",
        "from torch.optim import Adam\n",
        "from tqdm import tqdm\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++=\n",
        "tokenizer2 = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "labels = {'negative': 0,\n",
        "          'positive': 1\n",
        "          }\n",
        "\n",
        "def calc_metrics(y_test, y_pred):\n",
        "    c_mat = confusion_matrix(y_test, y_pred)\n",
        "    tn, fp, fn, tp = c_mat.ravel()\n",
        "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
        "    precision = (tp) / (tp + fp)\n",
        "    recall = (tp) / (tp + fn)\n",
        "    fscore = (2 * precision * recall) / (precision + recall)\n",
        "    print('accuracy: ', accuracy)\n",
        "    print('precision: ', precision)\n",
        "    print('recall: ', recall)\n",
        "    print('fscore: ', fscore)\n",
        "    print('confusion matrix: ')\n",
        "    print(c_mat, \"\\n\")\n",
        "    return\n",
        "\n",
        "class Dataset(utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, df):\n",
        "        self.labels = [labels[label] for label in df['sentiment']]\n",
        "        self.texts = [tokenizer2(text, padding='max_length', max_length=512, truncation=True, return_tensors=\"pt\") for\n",
        "                      text in df['review']]\n",
        "\n",
        "    def classes(self):\n",
        "        return self.labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def get_batch_labels(self, idx):\n",
        "        # Fetch a batch of labels\n",
        "        return np.array(self.labels[idx])\n",
        "\n",
        "    def get_batch_texts(self, idx):\n",
        "        # Fetch a batch of inputs\n",
        "        return self.texts[idx]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_texts = self.get_batch_texts(idx)\n",
        "        batch_y = self.get_batch_labels(idx)\n",
        "\n",
        "        return batch_texts, batch_y\n",
        "\n",
        "\n",
        "class BertClassifier(nn.Module):\n",
        "    def __init__(self, dropout=0.1):\n",
        "        super(BertClassifier, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained('bert-base-cased')\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        self.fn1 = nn.Linear(768, 512)\n",
        "        self.r1 = nn.ReLU()\n",
        "        \n",
        "        self.fn2 = nn.Linear(512, 256)\n",
        "        self.r2 = nn.ReLU()\n",
        "        \n",
        "        self.fn3 = nn.Linear(256, 128)\n",
        "        self.r3 = nn.ReLU()\n",
        "        \n",
        "        self.fn4 = nn.Linear(128, 64)\n",
        "        self.r4 = nn.ReLU()\n",
        "        \n",
        "        self.fn5 = nn.Linear(64, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "\n",
        "    def forward(self, input_id, mask):\n",
        "        _, pooled_output = self.bert(input_ids=input_id, attention_mask=mask, return_dict=False)\n",
        "        dropout_output = self.dropout(pooled_output)\n",
        "        linear_output = self.fn1(dropout_output)\n",
        "        linear_output = self.r1(linear_output)\n",
        "        linear_output = self.dropout(linear_output)\n",
        "\n",
        "        linear_output = self.fn2(linear_output)\n",
        "        linear_output = self.r2(linear_output)\n",
        "        linear_output = self.dropout(linear_output)\n",
        "\n",
        "        linear_output = self.fn3(linear_output)\n",
        "        linear_output = self.r3(linear_output)\n",
        "        linear_output = self.dropout(linear_output)\n",
        "\n",
        "        linear_output = self.fn4(linear_output)\n",
        "        linear_output = self.r4(linear_output)\n",
        "        linear_output = self.dropout(linear_output)\n",
        "\n",
        "        linear_output = self.fn5(linear_output)\n",
        "\n",
        "        proba = self.sigmoid(linear_output)\n",
        "        return proba\n",
        "        #return self.layers(dropout_output)\n",
        "\n",
        "\n",
        "def train(model, train_data, val_data, learning_rate, epochs, check):\n",
        "    train, val = Dataset(train_data), Dataset(val_data)\n",
        "    train_dataloader = utils.data.DataLoader(train, batch_size=1, shuffle=False)\n",
        "    val_dataloader = utils.data.DataLoader(val, batch_size=1)\n",
        "    model.eval()\n",
        "    print(len(train))\n",
        "    print(len(val))\n",
        "    print(len(train_dataloader))\n",
        "    print(len(val_dataloader))\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "    criterion = nn.BCELoss()\n",
        "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    if use_cuda:\n",
        "        model = model.cuda()\n",
        "        criterion = criterion.cuda()\n",
        "    train_arr = []\n",
        "    val_arr = []\n",
        "    index = 0\n",
        "    bestEpochTrain = 0\n",
        "    bestEpochValidate = 0\n",
        "    bestTrainAcc = 0\n",
        "    bestValAcc = 0\n",
        "    startRate = 1e-1\n",
        "    # to 1e-5\n",
        "    maxRateVal = 0\n",
        "    maxRateAcc = 0\n",
        "    arrRates = []\n",
        "    arrRatesAcc = []\n",
        "    epochesArr = []\n",
        "    if (check == 0) : # means improve epoches\n",
        "        for epoch_num in range(epochs):\n",
        "            total_acc_train = 0\n",
        "\n",
        "            for train_input, train_label in tqdm(train_dataloader):\n",
        "                train_label = train_label.to(device)\n",
        "                #train_label = train_label.to(torch.float32)\n",
        "\n",
        "                mask = train_input['attention_mask'].to(device)\n",
        "                input_id = train_input['input_ids'].squeeze(1).to(device)\n",
        "\n",
        "                output = model(input_id, mask)\n",
        "\n",
        "                output = torch.round(output)\n",
        "                acc = (output == train_label).sum().item()\n",
        "\n",
        "                total_acc_train += acc\n",
        "                \n",
        "\n",
        "                model.zero_grad()\n",
        "\n",
        "                optimizer.step()\n",
        "\n",
        "            total_acc_val = 0\n",
        "            \n",
        "\n",
        "            with torch.no_grad():\n",
        "\n",
        "                for val_input, val_label in val_dataloader:\n",
        "                    val_label = val_label.to(device)\n",
        "                    #val_label = val_label.to(torch.float32)\n",
        "\n",
        "                    mask = val_input['attention_mask'].to(device)\n",
        "                    input_id = val_input['input_ids'].squeeze(1).to(device)\n",
        "                    output = model(input_id, mask)\n",
        "\n",
        "                    output = torch.round(output)\n",
        "                    acc = (output == val_label).sum().item()\n",
        "                    total_acc_val += acc\n",
        "                \n",
        "\n",
        "            if (bestTrainAcc<(total_acc_train / len(train_data))) :\n",
        "                bestTrainAcc = total_acc_train / len(train_data)\n",
        "                bestEpochTrain = epoch_num + 1\n",
        "            if (bestValAcc<(total_acc_val / len(val_data))) :\n",
        "                bestValAcc = total_acc_val / len(val_data)\n",
        "                bestEpochValidate = epoch_num + 1\n",
        "                print(\"\\n\\nbestEpochValidate = \", bestEpochValidate)\n",
        "            train_arr.append(total_acc_train / len(train_data))\n",
        "            val_arr.append(total_acc_val / len(val_data))\n",
        "            epochesArr.append(epoch_num)\n",
        "            index = index + 1\n",
        "\n",
        "        plt.plot(epochesArr , train_arr )\n",
        "        plt.annotate(\"Maximum\" + \"(\" + str(bestEpochTrain) + \",\" + str(bestTrainAcc) + \")\", (bestEpochTrain, bestTrainAcc))\n",
        "        plt.show()\n",
        "        plt.plot(epochesArr, val_arr)\n",
        "        plt.annotate(\"Maximum\" + \"(\" + str(bestEpochValidate) + \",\" + str(bestValAcc) + \")\", (bestEpochValidate,bestValAcc))\n",
        "        plt.show()\n",
        "        return bestEpochValidate\n",
        "    elif (check == 1): # means improve learning rate\n",
        "        while(startRate >= (1e-5)):\n",
        "            total_acc_val = 0\n",
        "            with torch.no_grad():\n",
        "\n",
        "                for val_input, val_label in val_dataloader:\n",
        "                    val_label = val_label.to(device)\n",
        "                    #val_label = val_label.to(torch.float32)\n",
        "\n",
        "                    mask = val_input['attention_mask'].to(device)\n",
        "                    input_id = val_input['input_ids'].squeeze(1).to(device)\n",
        "                    output = model(input_id, mask)\n",
        "                    \n",
        "                    output = torch.round(output)\n",
        "                    acc = (output == val_label).sum().item()\n",
        "                    total_acc_val += acc\n",
        "            if (maxRateAcc < (total_acc_val / len(val_data))):\n",
        "                maxRateAcc = total_acc_val / len(val_data)\n",
        "                maxRateVal = startRate\n",
        "            arrRatesAcc.append(total_acc_val / len(val_data))\n",
        "            arrRates.append(startRate)\n",
        "            index = index + 1\n",
        "            startRate = startRate / 2\n",
        "        plt.plot(arrRates, arrRatesAcc)\n",
        "        plt.annotate(\"Maximum\" + \"(\" + str(maxRateVal) + \",\" + str(maxRateAcc) + \")\",\n",
        "                     (maxRateVal, maxRateAcc))\n",
        "        plt.show()\n",
        "        return maxRateVal\n",
        "    else :\n",
        "        olist = []\n",
        "        for epoch_num in range(epochs):\n",
        "\n",
        "            total_acc_train = 0\n",
        "            total_loss_train = 0\n",
        "\n",
        "            for train_input, train_label in tqdm(train_dataloader):\n",
        "\n",
        "                train_label = train_label.to(device)\n",
        "                train_label = train_label.to(torch.float32)\n",
        "                mask = train_input['attention_mask'].to(device)\n",
        "                input_id = train_input['input_ids'].squeeze(1).to(device)\n",
        "\n",
        "                output = model(input_id, mask)\n",
        "                \n",
        "\n",
        "                batch_loss = criterion(output, train_label.unsqueeze(1))\n",
        "                total_loss_train += batch_loss.item()\n",
        "                \n",
        "                output = torch.round(output)\n",
        "                acc = (output == train_label).sum().item()\n",
        "                total_acc_train += acc\n",
        "\n",
        "                model.zero_grad()\n",
        "                batch_loss.backward()\n",
        "                optimizer.step()\n",
        "            \n",
        "            total_acc_val = 0\n",
        "            total_loss_val = 0\n",
        "\n",
        "            with torch.no_grad():\n",
        "\n",
        "                for val_input, val_label in val_dataloader:\n",
        "\n",
        "                    val_label = val_label.to(device)\n",
        "                    val_label = val_label.to(torch.float32)\n",
        "                    mask = val_input['attention_mask'].to(device)\n",
        "                    input_id = val_input['input_ids'].squeeze(1).to(device)\n",
        "\n",
        "                    output = model(input_id, mask)\n",
        "\n",
        "                    batch_loss = criterion(output, val_label.unsqueeze(1))\n",
        "                    total_loss_val += batch_loss.item()\n",
        "                    \n",
        "                    output = torch.round(output)\n",
        "                    acc = (output == val_label).sum().item()\n",
        "                    total_acc_val += acc\n",
        "            \n",
        "            print(\n",
        "                f'Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train_data): .3f} \\\n",
        "                | Train Accuracy: {total_acc_train / len(train_data): .3f} \\\n",
        "                | Val Loss: {total_loss_val / len(val_data): .3f} \\\n",
        "                | Val Accuracy: {total_acc_val / len(val_data): .3f}')\n",
        "            y_output = pd.DataFrame (olist, columns = ['sentiment'])\n",
        "            #calc_metrics(val_data[['sentiment']], y_output)\n",
        "            olist = []\n",
        "        \n",
        "        \n",
        "# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "\n",
        "url='https://drive.google.com/file/d/17KCPXOPTYXx9zDqk1GtouAL-mgQalGiY/view?usp=sharing'\n",
        "url='https://drive.google.com/uc?id=' + url.split('/')[-2]\n",
        "unproc_df = pd.read_csv(url)\n",
        "\n",
        "# ++++++++++++++++++++++\n",
        "unproc_df = unproc_df.sample(n=300, replace=False, random_state = 42)\n",
        "# ++++++++++++++++++++++\n",
        "\n",
        "# Pre-processing the data\n",
        "# ======================================================================================================================================\n",
        "proc_df = unproc_df.copy(deep = True)\n",
        "\n",
        "# Lowercase all characters\n",
        "proc_df['review'] = proc_df.apply(lambda row: (row['review']).lower(), axis = 1)\n",
        "\n",
        "# Remove punctuation\n",
        "tokenizer = RegexpTokenizer(r\"\\w+\")\n",
        "proc_df['review'] = proc_df.apply(lambda row: ' '.join(tokenizer.tokenize(row['review'])), axis = 1)\n",
        "\n",
        "# Remove stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "proc_df['review'] = proc_df.apply(lambda row: ' '.join([w for w in word_tokenize(row['review']) if not w in stop_words]), axis = 1)\n",
        "\n",
        "# Lemmatization of words\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "proc_df['review'] = proc_df.apply(lambda row: ' '.join([lemmatizer.lemmatize(w) for w in word_tokenize(row['review'])]), axis = 1)\n",
        "\n",
        "# print(unproc_df.head())\n",
        "# print(proc_df.head())\n",
        "\n",
        "# End of pre-processing # ==============================================================================================================\n",
        "\n",
        "# Separating the data based on sentiment i.e positive or negative\n",
        "unproc_positive = unproc_df[unproc_df.iloc[:, -1] == 'positive'];\n",
        "unproc_negative = unproc_df[unproc_df.iloc[:, -1] == 'negative'];\n",
        "\n",
        "proc_positive = proc_df[proc_df.iloc[:, -1] == 'positive'];\n",
        "proc_negative = proc_df[proc_df.iloc[:, -1] == 'negative'];\n",
        "\n",
        "# Sampling 70%, 10% and 20% for training, validation and testing respectively from each class, positive and negative\n",
        "np.random.seed(100)\n",
        "df_train_p1, df_val_p1, df_test_p1 = np.split(unproc_positive.sample(frac=1, random_state=42), [int(.7*len(unproc_positive)), int(.8*len(unproc_positive))])\n",
        "df_train_p2, df_val_p2, df_test_p2 = np.split(unproc_negative.sample(frac=1, random_state=42), [int(.7*len(unproc_negative)), int(.8*len(unproc_negative))])\n",
        "\n",
        "np.random.seed(100)\n",
        "df_train_p3, df_val_p3, df_test_p3 = np.split(proc_positive.sample(frac=1, random_state=42), [int(.7*len(proc_positive)), int(.8*len(proc_positive))])\n",
        "df_train_p4, df_val_p4, df_test_p4 = np.split(proc_negative.sample(frac=1, random_state=42), [int(.7*len(proc_negative)), int(.8*len(proc_negative))])\n",
        "# Concatinating the positive and negative datasets, thus we have balanced sampling for training, validation and testing for both pre-processed and unprocessed data\n",
        "df_train_unproc = pd.concat([df_train_p1, df_train_p2]);\n",
        "df_val_unproc = pd.concat([df_val_p1, df_val_p2]);\n",
        "df_test_unproc = pd.concat([df_test_p1, df_test_p2]);\n",
        "\n",
        "df_train_proc = pd.concat([df_train_p3, df_train_p4]);\n",
        "df_val_proc = pd.concat([df_val_p3, df_val_p4]);\n",
        "df_test_proc = pd.concat([df_test_p3, df_test_p4]);\n",
        "\n",
        "print(\"\\n\\n\",len(df_train_unproc[df_train_unproc['sentiment'] == 'positive']))\n",
        "print(\"\\n\\n\",len(df_train_unproc[df_train_unproc['sentiment'] == 'negative']))\n",
        "\n",
        "EPOCHS = 5\n",
        "model = BertClassifier()\n",
        "LR = 1e-3\n",
        "\n",
        "bestEpoch = train(model, df_train_proc, df_val_proc, LR , EPOCHS, 0)\n",
        "bestRate = train(model, df_train_proc, df_val_proc, LR , bestEpoch , 1)\n",
        "print(\"\\nbest epoch = \", bestEpoch)\n",
        "print(\"\\nbest rate = \", bestRate)\n",
        "train(model, df_train_proc, df_val_proc, bestRate, bestEpoch , 2)\n",
        "# '''\n",
        "# bestEpoch2 = train(model, df_train_unproc, df_val_unproc, LR , EPOCHS, 0)\n",
        "# bestRate2 = train(model, df_train_unproc, df_val_unproc , LR , bestEpoch2 , 1)\n",
        "# train(model, df_train_unproc,df_val_unproc, bestRate2 , bestEpoch2 , 2)\n",
        "# '''\n",
        "# print(len(df_train_unproc))\n",
        "# print(len(df_val_unproc))\n",
        "# print(len(df_test_unproc))\n",
        "\n",
        "# print(len(df_train_proc))\n",
        "# print(len(df_val_proc))\n",
        "# print(len(df_test_proc))\n",
        "\n",
        "# print(df_train_unproc.head())\n",
        "# print(df_val_unproc.head())\n",
        "# print(df_test_unproc.head())\n",
        "\n",
        "# print(df_train_proc.head())\n",
        "# print(df_val_proc.head())\n",
        "# print(df_test_proc.head())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "wnJs2zSX0f64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "wOwhdWWCNn__"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}